# Mini-RAG-BOOT

RAG Flow Implementation

The RAG pipeline is implemented in the backend as follows:

1. Text Ingestion

Input can be either:

Uploaded text file

Pasted text from the UI

Both are handled by the same /upload endpoint.

2. Chunking Strategy

The document is split using line-based chunking.

This approach was chosen because the input documents are often notes, bullet points, or structured text rather than clean prose.

Line-based chunking avoids:

Broken words

Incomplete sentences

Poor semantic grouping

3. Embedding Generation

Each chunk is converted into a vector embedding using:

sentence-transformers/all-MiniLM-L6-v2

Embeddings are stored in an in-memory list for simplicity.

4. Similarity Search

When a question is asked:

The question is converted into an embedding.

Cosine similarity is used to compare it with stored chunk embeddings.

The top-K most relevant chunks are retrieved.

5. Answer Generation (Mocked LLM)

Instead of calling an external LLM API, a mocked LLM approach is used.

The answer is generated by:

Extracting relevant sentences from retrieved chunks

Deduplicating overlapping sentences

This ensures:

No hallucinations

Answers are strictly grounded in the document

Focus remains on understanding RAG rather than model APIs

6. Transparency

Along with the answer, the backend returns the supporting document chunks.

These are shown in the UI so users can see where the answer came from.

How to Run the Project Locally
Prerequisites

Python 3.10+

Node.js (if using React frontend)

npm
